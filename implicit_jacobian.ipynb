{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Check of Theorem\n",
    "Reacting to the reviewers request, we numerically check our derivation of the state-to-state jacobian.\n",
    "\n",
    "Approach: Iterate a single token till convergence in a random model with random weights and a random initial hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_deq.models import ImplicitModel, ExplicitModel\n",
    "from omegaconf import OmegaConf as om\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "\n",
    "# TODO\n",
    "# - 64 bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "deq_params = om.load('config/deq/phantom.yaml')\n",
    "deq_params.solver.f_tol=1e-6\n",
    "deq_params.solver.f_max_iter=512\n",
    "deq_params.solver.eval_f_max_iter=512\n",
    "\n",
    "precision = torch.float64\n",
    "d_model = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImplicitModel(\n",
    "    deq_params, \n",
    "    d_model=d_model,\n",
    "    n_layer=1,\n",
    "    d_inner=0,\n",
    "    pre_norm=True,\n",
    "    pretrain_steps=0,\n",
    "    block_cfg={\n",
    "        'd_state': 1,\n",
    "        'expand': 1,\n",
    "        'headdim': d_model,\n",
    "    },\n",
    "    device='cuda',\n",
    "    dtype=precision\n",
    ")\n",
    "explicit_model = ExplicitModel(\n",
    "    d_model=d_model,\n",
    "    n_layer=1,\n",
    "    d_inner=0,\n",
    "    pre_norm=True,\n",
    "    block_cfg={\n",
    "        'd_state': 1,\n",
    "        'expand': 1,\n",
    "        'headdim': d_model,\n",
    "    },\n",
    "    device='cuda',\n",
    "    dtype=precision\n",
    ")\n",
    "explicit_model.layers = model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "device = 'cuda'\n",
    "initial_state = model.allocate_inference_cache(batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, d_model).to(device, precision)\n",
    "u = torch.randn(1, 2 * d_model + 3).to(device, precision)\n",
    "z = torch.zeros(1, d_model).to(device, precision)\n",
    "conv_state = torch.rand_like(initial_state[0][0]).to(device, precision)\n",
    "h = torch.rand(1, 1, d_model, 1).to(device, precision) * 2\n",
    "model = model.to(device, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_cache(h):\n",
    "    return [(conv_state, h)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Explicit and Implicit Model Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(h):\n",
    "    inference_cache = get_inference_cache(h)\n",
    "    fixed_point, new_inference_cache, a, r, s = model._sequential_step(z, u, inference_cache)\n",
    "    new_h = new_inference_cache[0][1]\n",
    "    print(f'Convergence after {s} steps with rel diff {r}')\n",
    "    return new_h, fixed_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fixed point\n",
    "with torch.no_grad():\n",
    "    _, z_fp = step(h)\n",
    "\n",
    "J = jacobian(lambda x: step(x)[0], h, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_explicit(h):\n",
    "    inference_cache = get_inference_cache(h)\n",
    "    output, new_inference_cache = model._step(x, injected_inputs=None, inference_cache=inference_cache)\n",
    "    new_h = new_inference_cache[0][1]\n",
    "    return new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_explicit = jacobian(step_explicit, h, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "data1 = np.abs(J.squeeze().cpu().numpy())\n",
    "data2 = np.abs(J_explicit.squeeze().cpu().numpy())\n",
    "\n",
    "# Use the same normalization across both plots\n",
    "norm = LogNorm(vmin=1e-4, vmax=1)\n",
    "print(norm.vmin, norm.vmax)\n",
    "img1 = axes[0].imshow(data1, norm=norm)\n",
    "img2 = axes[1].imshow(data2, norm=norm)\n",
    "\n",
    "# One colorbar for both axes\n",
    "fig.colorbar(img2, ax=axes, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "# Annotations\n",
    "axes[0].set_title('Implicit Jacobian')\n",
    "axes[1].set_title('Explicit Jacobian')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('State index')\n",
    "    ax.set_ylabel('State index')\n",
    "fig.subplots_adjust(left=0.1, right=0.8, wspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "bins = np.logspace(-8, 0, 20)\n",
    "ax.hist(J.squeeze().cpu().numpy().flatten(), bins=bins)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Theorem Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Mamba Internal Variables\n",
    "We need to get derivatives of $\\Lambda$ and $u$ from the Mamba code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024, Tri Dao, Albert Gu.\n",
    "import math\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def mamba(self, hidden_states, injected_inputs, conv_state, ssm_state) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"\n",
    "        Sequentially step through a sequence and carry over internal states for convolutions and state-space models.\n",
    "        Args:\n",
    "            hidden_states: input to the Mamba2 layer (B, D_model)\n",
    "            injected_inputs: injected input to the Mamba2 layer (B, D_in_proj)\n",
    "            conv_state: carry for convolution (B, D_conv, W)\n",
    "            ssm_state: carry for state-space model (B, nheads, headdim, D_state)\n",
    "\n",
    "        Returns:\n",
    "            out: output for this step (B, D_model)\n",
    "            new_conv_state: updated convolution state (B, D_conv, W)\n",
    "            new_ssm_state: updated state-space model state (B, nheads, headdim, D_state)1\n",
    "\n",
    "        \"\"\"\n",
    "        dtype = hidden_states.dtype\n",
    "\n",
    "        if hidden_states.dim() > 2:\n",
    "            assert hidden_states.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
    "            hidden_states = hidden_states.squeeze(1)\n",
    "        zxbcdt = self.in_proj(hidden_states)  # (B 2D)\n",
    "\n",
    "        # inject inputs\n",
    "        if injected_inputs is not None:\n",
    "            if injected_inputs.dim() > 2:\n",
    "                assert injected_inputs.shape[1] == 1, \"Only support decoding with 1 token at a time for now\"\n",
    "                injected_inputs = injected_inputs.squeeze(1)\n",
    "            zxbcdt += injected_inputs\n",
    "\n",
    "        d_mlp = (zxbcdt.shape[-1] - 2 * self.d_ssm - 2 * self.ngroups * self.d_state - self.nheads) // 2\n",
    "        z0, x0, z, xBC, dt = torch.split(\n",
    "            zxbcdt, [d_mlp, d_mlp, self.d_ssm, self.d_ssm + 2 * self.ngroups * self.d_state, self.nheads], dim=-1\n",
    "        )\n",
    "\n",
    "        # Conv step\n",
    "        new_conv_state = torch.roll(conv_state, shifts=-1, dims=-1)  # Update state (B D W)\n",
    "        new_conv_state[:, :, -1] = xBC\n",
    "        xBC = torch.sum(new_conv_state * rearrange(self.conv1d.weight, \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
    "        if self.conv1d.bias is not None:\n",
    "            xBC = xBC + self.conv1d.bias\n",
    "        xBC = self.act(xBC).to(dtype=dtype)\n",
    "\n",
    "        x, B, C = torch.split(xBC, [self.d_ssm, self.ngroups * self.d_state, self.ngroups * self.d_state], dim=-1)\n",
    "        A = -torch.exp(self.A_log.float())  # (nheads,)\n",
    "\n",
    "        # SSM step\n",
    "        assert self.ngroups == 1, \"Only support ngroups=1 for this inference code path\"\n",
    "        # Discretize A and B\n",
    "        dt = F.softplus(dt + self.dt_bias.to(dtype=dt.dtype))  # (batch, nheads)\n",
    "        dA = torch.exp(dt * A)  # (batch, nheads)\n",
    "        x = rearrange(x, \"b (h p) -> b h p\", p=self.headdim)\n",
    "        dBx = torch.einsum(\"bh,bn,bhp->bhpn\", dt, B, x)\n",
    "        new_ssm_state = ssm_state * rearrange(dA, \"b h -> b h 1 1\") + dBx\n",
    "        y = torch.einsum(\"bhpn,bn->bhp\", new_ssm_state.to(dtype), C)\n",
    "        y = y + rearrange(self.D.to(dtype), \"h -> h 1\") * x\n",
    "        y = rearrange(y, \"b h p -> b (h p)\")\n",
    "        if not self.rmsnorm:\n",
    "            y = y * self.act(z)  # (B D)\n",
    "        if self.rmsnorm:\n",
    "            y = self.norm(y, z)\n",
    "        if d_mlp > 0:\n",
    "            y = torch.cat([F.silu(z0) * x0, y], dim=-1)\n",
    "        out = self.out_proj(y)\n",
    "\n",
    "        return out, dA, dBx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba(model.layers[0].mixer, z_fp, u, *initial_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(self, z, u, inference_cache=None):\n",
    "    # split off residual\n",
    "    residual = z\n",
    "\n",
    "    # pre-norm formulation\n",
    "    if self.pre_norm:\n",
    "        z = self.norm(z.to(dtype=self.norm.weight.dtype))\n",
    "\n",
    "    # apply the time mixer (SSM / Transformer) and skip connection\n",
    "    inference_cache_t = inference_cache if isinstance(inference_cache, tuple) else (inference_cache,)\n",
    "    z, dA, dBx = mamba(self.mixer, z, u, *inference_cache_t)\n",
    "    z = residual + z\n",
    "    if self.residual_in_fp32:\n",
    "        z = z.to(torch.float32)\n",
    "    return z, dA, dBx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_func(z, h):\n",
    "    inference_cache = get_inference_cache(h)[0]\n",
    "    out, A, Bx = block(model.layers[0], z, u, inference_cache)\n",
    "    return model.norm_f(out), A, Bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model outputs match and fixed point is indeed found\n",
    "# out, _, _ = model_func(z_fp, h)\n",
    "# iter_out, _, _, _, s = model._sequential_step(z_fp, u, get_inference_cache(h))\n",
    "# print(torch.cat([z_fp, iter_out, out], dim=0)[:,:8])\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, A, Bx = model_func(z_fp, h)\n",
    "jac = jacobian(model_func, (z_fp, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dzdz = jac[0][0].squeeze()\n",
    "dzdh = jac[0][1].squeeze()\n",
    "dAdz = jac[1][0].squeeze()\n",
    "dBxdz = jac[2][0].squeeze()\n",
    "print(dzdz.shape, dzdh.shape, dAdz.shape, dBxdz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = torch.eye(d_model).to(device) - dzdz\n",
    "print('dzdz has rank', torch.linalg.matrix_rank(G).item())\n",
    "print('dzdz condition number:', torch.linalg.cond(G).item())\n",
    "dfdh = torch.einsum('ij,jk->ik', torch.inverse(G), dzdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "theorem = torch.eye(d_model).to(device) * A + torch.einsum('k,kj,i->ij', dAdz, dfdh, h.squeeze()) + torch.einsum('ij,jk->ik', dBxdz, dfdh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "\n",
    "data1 = J.squeeze().cpu().numpy()\n",
    "data2 = theorem.squeeze().cpu().numpy()\n",
    "diff  = data1 - data2\n",
    "\n",
    "# Use the same normalization across both plots\n",
    "print(data1.min(), data2.min(), data1.max(), data2.max())\n",
    "print(diff.min(), diff.max())\n",
    "norm = Normalize(vmin=-0.1, vmax=0.1)\n",
    "lognorm = LogNorm(vmin=1e-8, vmax=1)\n",
    "cmap = 'coolwarm'\n",
    "img1 = axes[0].imshow(data1, cmap=cmap, norm=norm)\n",
    "img2 = axes[1].imshow(data2, cmap=cmap, norm=norm)\n",
    "img3 = axes[2].imshow(np.abs(diff), norm=lognorm)\n",
    "\n",
    "# One colorbar for both axes\n",
    "fig.colorbar(img1, ax=axes[0], orientation='horizontal', fraction=0.046, pad=0.2)\n",
    "fig.colorbar(img2, ax=axes[1], orientation='horizontal', fraction=0.046, pad=0.2)\n",
    "fig.colorbar(img3, ax=axes[2], orientation='horizontal', fraction=0.046, pad=0.2)\n",
    "\n",
    "# Annotations\n",
    "axes[0].set_title('Autograd Jacobian')\n",
    "axes[1].set_title('Formula Jacobian')\n",
    "axes[2].set_title('Difference')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('State index')\n",
    "    ax.set_ylabel('State index')\n",
    "fig.subplots_adjust(left=0.1, right=0.8, wspace=0.4)\n",
    "fig.savefig('implicit_jacobian_check.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "bins = np.logspace(-8, 0, 20)\n",
    "axes[0].hist(data1.flatten(), bins=bins)\n",
    "axes[1].hist(diff.flatten(), bins=bins)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [data1, data2, diff]:\n",
    "    print(np.median(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
