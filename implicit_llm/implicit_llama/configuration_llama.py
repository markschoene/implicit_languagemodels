from dataclasses import asdict, dataclass, field
from typing import Any

from transformers import PretrainedConfig


@dataclass
class SolverConfig:
    f_solver: str
    b_solver: str
    f_max_iter: int
    b_max_iter: int
    f_tol: float
    b_tol: float
    f_stop_mode: str
    b_stop_mode: str
    eval_factor: float
    eval_f_max_iter: int


@dataclass
class NormConfig:
    norm_type: str
    norm_no_scale: bool
    norm_clip: bool
    norm_clip_val: float
    norm_target_norm: float
    sn_n_power_iters: int


@dataclass
class TrainingConfig:
    core: str
    ift: bool
    hook_ift: bool
    n_states: int
    indexing: list
    gamma: float
    grad: int
    tau: float
    sup_gap: int
    sup_loc: list


@dataclass
class RegularizationConfig:
    jac_loss_weight: float
    jac_loss_freq: float
    jac_incremental: int
    sradius_mode: bool


@dataclass
class DEQConfig:
    solver: dict
    norm: dict
    training: dict
    regularization: dict

    @staticmethod
    def from_dict(deq_dict):
        solver_dict = asdict(SolverConfig(**deq_dict["solver"]))
        norm_dict = asdict(NormConfig(**deq_dict["norm"]))
        training_dict = asdict(TrainingConfig(**deq_dict["training"]))
        regularization_dict = asdict(RegularizationConfig(**deq_dict["regularization"]))
        return DEQConfig(solver=solver_dict, norm=norm_dict, training=training_dict, regularization=regularization_dict)


@dataclass
class BackboneConfig:
    n_layer: int
    d_model: int
    d_inner: int
    pre_norm: bool
    rms_norm: bool
    dropout: float
    pretrain_iter: int
    pretrain_steps: int
    init_gain: float
    block_type: str
    block_cfg: dict[str, Any] = field(default_factory=dict)


@dataclass
class CausalLMConfig:
    tie_embeddings: bool
    pad_vocab: bool
    pad_vocab_size_multiple: int
    d_embed: int
    emb_init_std: float
    dropout: float
    weight_decay: float
    word_level: bool
    backbone: BackboneConfig
    data_info: Any = field(default_factory=dict)
    vocab_size: int | None = None

    @staticmethod
    def from_dict(config_dict) -> "CausalLMConfig":
        backbone_config = BackboneConfig(**config_dict["backbone"])

        config_dict["backbone"] = backbone_config

        if "deq" in config_dict["backbone"].deq_params:
            deq_config = DEQConfig(**config_dict["backbone"].deq_params["deq"])
            config_dict["backbone"].deq_params = deq_config

        return CausalLMConfig(**config_dict)


class ImplicitLlamaConfig(PretrainedConfig):
    model_type = "implicit_llama3"

    def __init__(
        self,
        deq_params: dict = {},
        backbone_type: str = "implicit",
        backbone_config: dict = {},
        vocab_size: int = 50277,
        head_bias: bool = False,
        d_embed: int | None = None,
        average_eval: bool = True,
        dropout: float = 0.05,
        weight_decay: float = 0.0,
        data_info: dict = {},
        dataset_name: str | None = None,
        pad_vocab: bool = False,
        pad_vocab_size_multiple: int = 8,
        tie_embeddings: bool = True,
        emb_init_std: float = 0.01,
        keep_sequence_dim: bool = False,
        save_output_ids: bool = False,
        **kwargs,
    ):
        self.deq_params = deq_params
        self.vocab_size = vocab_size
        self.backbone_type = backbone_type
        self.backbone_config = backbone_config
        self.head_bias = head_bias
        self.d_embed = d_embed
        self.average_eval = average_eval
        self.dropout = dropout
        self.weight_decay = weight_decay
        self.data_info = data_info
        self.dataset_name = dataset_name
        self.pad_vocab = pad_vocab
        self.pad_vocab_size_multiple = pad_vocab_size_multiple
        self.tie_embeddings = tie_embeddings
        self.emb_init_std = emb_init_std
        self.keep_sequence_dim = keep_sequence_dim
        self.save_output_ids = save_output_ids
        self.tokenizer = "EleutherAI/gpt-neox-20b"

        super().__init__(**kwargs)


if __name__ == "__main__":
    pretrained_model_config_name = "mamba2-1.3b-wt"
    my_config = ImplicitLlamaConfig.from_pretrained(pretrained_model_config_name)
    my_config.save_pretrained(f"{pretrained_model_config_name}")
